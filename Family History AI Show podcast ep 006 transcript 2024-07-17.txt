<TRANSCRIPT>

<METADATA>
PODCAST: "The Family History AI Show" podcast
EPISODE: 006
HOSTS: Mark Thompson and Steve Little
DATE: 2 July 2024 (recorded); aired: 16 July 2024
LICENSE: This work by "The Family History AI Show" is licensed under a Creative Commons BY-NC 4.0 License.
</METADATA>

<RAW TRANSCRIPT>
Welcome to the Family History AI Show. My name is Mark Thompson. I'm here with my co-host, Steve Little. We're excited to have you join us for a weekly discussion of the most interesting topics in AI, and in particular, how they are affecting the family history community. In this week's headlines, we're going to talk about a great new tool from Anthropic in Claude called Projects. And after that, we're going to talk about the questions that genealogy students have about artificial intelligence, the questions that they are most interested in. After that, we will dive into some of the most frequently asked questions from those same students this year at the GRIP Annual Institute in Artificial Intelligence. And finally, we will wrap up with this week's tip of the week, where we talk about why it is so important to chat with your chatbot. So let's jump into this week's first story, Steve. Claude Projects quietly rolled out the last two or three weeks along with the Big Sonnet 3.5 update at about the same time. And we finally had a chance to take a look at it and do a little bit of testing. Claude Projects really almost slipped past me at first because the initial reports that I heard about it sounded like it was perhaps something only for big business. We were in the middle of, you know, teaching 18 classes in four and a half days. I didn't have a whole lot of extra time. And so it actually slipped past me for a couple days how big a deal this is. Of course, Anthropic made big headlines a couple weeks ago with the release of Claude 3.5 Sonnet, which is a step ahead in reasoning and ability. And now this is the equivalent of reusable prompt, the equivalent of OpenAI's custom GPTs. The ability to save and reuse prompts and files is a big deal. When I read the blurb on this new product, I read enterprise tools work between more than one person. And I didn't think it was going to be super relevant for genealogists. But after testing it for a few days, finally getting a chance to take a look at it, I agree. This is actually, this is the first time we've, I think, been able to reasonably say there is another reusable prompt option in town other than custom GPTs from ChatGPT. This is a major validation of something that we've been talking about since November. It was on November 6th that OpenAI first announced this way to save prompts and save data and reuse them quickly and easily. Being able to make the tools that you need is a big deal. And OpenAI began that in November with their way to reuse prompts and files called custom GPTs. And now we have a major validation of that from Anthropic. The power that I get from being able to go back and pick up that tool out of my toolkit and use it very, very quickly and easily, even as some other tools, some of the other tools like Claude and Gemini, Perplexity, have rolled out new features that are really, really useful. And in some cases, just the LLM is just better. It's so easy to just pick up and grab one of those custom GPTs off of the shelf and use it quickly for the things that I do all the time in my genealogy work. I keep going back to it. And I'm so happy now that Claude has a capability like that. Off the top of our heads, let's see if we can't name six of these concrete tools that we use. So, for example, you have an obituary tool. And we both probably have photograph analysis tools. And we both probably have transcription tools where you drop a birth certificate or any kind of record. Yeah. I have some summarizers that I use for letters research all the time. So these, when we talk about little tools in the toolbox, this is what we're talking about, special custom-made tools to summarize, to extract transcriptions, to analyze an image. I've got one that actually writes things. You give it a small list of names, dates, places, relationships, and events. And it will take a small list of data and spin that into a narrative report. It's a report generator. There's another one that I use all the time, too, where I just, I look through a document and I pull out all of the names and turn it into a comma-delimited list. So I can just pull all the names into a spreadsheet for quick and dirty analysis or checking stuff. I've got a similar one I call Steve's Fact Extractor. You can drop in any kind of document. It could be an obituary, a wedding announcement, and it goes through and just extracts facts. It just pulls out the raw facts from an article. You can always build yourself a little prompt library in a Word document or an Excel document or OneNote or Evernote, something like that. But having them available inside a custom GPT, in ChatGPT, the time it takes to actually start using one goes from a minute or two, you've got to go find it and copy and paste it, to literally five seconds. And that itself is a time saver, because if you use one of those custom GPTs 15 or 20 times a week, two minutes adds up. You save two minutes times 15 times, you saved a half an hour just because it's easy to use. And not only is it easy and quick to use, but it gets better over time if you find ways to slightly revise your prompt. The second big reason these reusable prompts are a big deal is mitigation of hallucinations. The antidote against hallucinations is three things. Know your data, know your model, know your limit, and being able to supply data. You know, whether it's you're dropping in an obituary, that's the data that you're giving to it. Or sometimes the data may be built into the reusable prompt. If you have writing guidelines or a stylistic manual that you want to always be available when you're generating something, having that always there is a big time saver too. Maybe let's take a closer look now at how Anthropx projects are similar and different from OpenAI's custom GPTs, because there's about two-thirds overlap. But there's some things that Anthropx projects are missing. But there's one or two things that Anthropx project actually does better, I think, than OpenAI's custom GPTs. The big shared feature of both is the ability to save your prompts. It could be as simple as, you know, talk like a pirate, or be verbose, or be concise. But it can also be much more elaborate than that, where you're giving multi-step instructions on how I want you to process a document in these five steps to extract the information and present it this way. OpenAI's custom GPT, they call that instructions. In Anthropx projects, they call it custom instructions. Creating them is a little bit easier in OpenAI's custom GPTs. They have an interview process where a ranked beginner can just be interviewed by the tool, and it will actually talk you through the process of creating itself. Yeah, that was something I really liked about, I still like about custom GPTs. That first five times or so that you made a custom GPT, it was kind of nice to be able to walk through it and see how the types of things it wanted to include in a GPT made it much more accessible when, you know, you first started using that tool. And they both have ways that you can go through afterwards and edit, or update, or revise, or improve. Both of these tools allow you to upload files for reference, and that's a big deal. My initial test showed that Anthropx projects are returning better results when I quiz information that's been uploaded. Yeah, it's kind of hard to tell, you know, they don't really say how they work. But I found the same thing, I found it referenced the attached documents better. One other thing that I noticed that Anthropx projects does better is the length of the prompts that you can create. In OpenAI's custom GPTs, the limit is it's an 8,000 context window for the instructions, which is just a couple pages. In Anthropx projects, I think I dropped a 20-page prompt into there, and I think it's dealing with the whole thing. Another couple similarities and differences between OpenAI's custom GPTs and Anthropx projects, web browsing. It does not appear that Anthropx projects can reach out to a live web page the way that a custom GPT can, but that's probably not a great loss. Similarly, custom GPTs in OpenAI have DALI 3 access. There's no image generation at Anthropx at this point, so, you know, we don't have that. A similar loss at OpenAI in their custom GPTs, you have your Python code writer and executor. You only get half of that with Anthropx projects. It's very good at writing code, but it does not execute Python code. It does lots of other cool stuff, just like OpenAI's custom GPTs have access to Data Analyst, and so it can create lots of cool charts and graphs. Anthropx does that too with what they call their artifacts. One of the great strengths of OpenAI is the ability to share your prompts with others. If I wanted to process an obituary with Mark's obituary tool, he can share that with me. Can't do that yet with Anthropx projects, but I'm sure that'll be coming. We'll go back to Anthropx, you know, special sauce. One of their core brand points is it's about security, right, safe information. I go back and forth about whether or not they're just going to take some time to figure out how to implement shareable projects easily, or if they're going to hold on to it for a while because there's a whole bunch of security issues that get created there. If Google is too highly guardrail and OpenAI is too loosey-goosey, Wild West, Anthropx is going for the middle ground, looking for the balance of safety and robustness. And so they're okay taking their time. But also, I'm not sure how many people actually are sharing their prompts. OpenAI GPT store, there's popular prompts there. And among genealogists, we share some of our custom GPTs, but it's not like any have become runaway bestsellers that you're hearing about anywhere. Maybe the demand for sharing these things is still building. I think it's going to be a big deal. But for right now, we seem to all have our own specialty tools. I think for some folks, the idea of just sharing reusable prompts is still pretty new. And there'll be some sort of industries that it makes a lot of sense to do it. I think there's so many people that are concerned about information sharing. They want to make sure that it doesn't get loaded into the training data. And maybe they're working on a project that has private information. So I think there is a little bit of sharing hesitancy, but there's so many things that genealogists do consistently day in, day out. I think and hope that there'll be more sharing of custom GPTs from ChatGPT or projects from Claude should that feature become available in the future. I did notice one other thing that I thought was really interesting about projects that distinguishes it from custom GPTs. When I make a custom GPT, like my obituary analyst, and all of the times that I call that custom GPT, it ends up in my GPT list. But I can't go back and they're kind of hard to find. My historical chat list in ChatGPT gets kind of unwieldy. But the nice thing I found about projects in Claude is that every time you call that project with a new chat, it actually stores them all together along with the project. So if I went back, it was really easy to see the five times that I called my obituary analyst project, all of those obituary reviews are all stored together. So it was kind of very efficient and very natural to go and find not only the project, but also all of the times that I used it and all of the responses that I'd got, they'd all in one place. So it kind of really goes back to that idea of a project. Things are encapsulated together and easy to get to and efficient to review way easier than inside ChatGPT. So that was actually one feature I really liked about it. Strongly agree. That's such a big improvement. I'll actually be surprised if OpenAI doesn't steal that idea very quickly, because you're right. Our chat histories in ChatGPT are unwieldy. In all of them, actually. In all of them, it's kind of unwieldy. I've noticed that on your phone, if you use the phone application, at least on an Android for ChatGPT, you can search your chat histories, but you cannot do that through the web-based version. I keep looking for a search function. It's so obvious, like a free text search of all of the words across all of your prompts. That would be so helpful. Anthropic may have solved this problem by grouping your chats with the saved prompt. For example, where this is going to benefit me is photograph analysis, because I've got two or three photography analysis prompts that I reuse, but having all of those gathered together in one place is bigly helpful. Very bigly. The next big item in the news was we had a week-long of instruction at what used to be called the Genealogical Research Institute of Pittsburgh, GRIP, now associated with the National Genealogical Society. I had the honor of leading seven colleagues and myself, taught 18 classes last week, and it was a phenomenal week. One of the things we closed on is a panel discussion, and in preparation for the panel discussion, I asked our 60 students to submit questions, and we actually received hundreds of questions. I used some artificial intelligence to organize and catalog the questions and group them together, and they ended up pooling together in four large pools. I thought maybe we'd take a look at what questions from 60 students who invested significant time and treasure to spend a week of their summer investigating this. These are the questions that inquiring minds want to know, Mark. I thought this was really interesting, because here's 60 people who have taken currently one of the most advanced classes that you can take in artificial intelligence and genealogy, and at the end of the week, took all of the questions from all of them and lumped them together into several buckets. It's a little bit of a sneak peek into what people who have actually studied this care about once they've studied it deeply. I thought this might be very, very interesting for our listeners to hear what AI genealogists care about after they've done some studying on it. We ended up with four big buckets. One was, what are the tools and the applications that people use the most? That was probably the number one most asked question. The second topic that people are most interested in was, how do I go about learning more and keeping up to date primarily about this rapidly changing field? And then number three was, how do I deal with the whole variety of ethical and potentially legal considerations in how I use these tools, as well as how I use the information that I get from these tools? And then finally, there was a whole variety of questions which got rolled up into another category around integration and the use of these tools in actual genealogy work. In other words, how do I incorporate these tools efficiently into the genealogy work that I do? So I thought this was like four really relevant groupings of types of questions that people really care about in genealogy, and I think are probably pretty representative of the types of things that the population of people are interested in, particularly once they kind of get through the beginner stages of how do these tools work. But once they're actually often using them, which is where these people were, these are the types of things that become top of mind. We want to take a crack at answering some of these questions or just leave folks hanging? Well, I thought it might be interesting. The topic that was the most asked was around tools and applications. So I thought it might be interesting for us to go through what I think is probably the most frequently asked question that I get, which is which LLM is best for what? A nice way to take this might be to walk through sort of the big three large language models, CHAT-GPT, CLAWD, and GemIIni, and kind of talk about what we think each one is good for and summarize a little bit about what all of the panelists thought. So maybe let's start with the great granddaddy AI tool of them all, CHAT-GPT. I think there's a whole genre of AI influencer who is just paid to announce the premature death of CHAT-GPT and open AI. Or whatever tool is at the top. It still is my first recommendation. If someone asks me still today, and we're recording on January 4th, 2024. Correction. Did I say January? You said January. Oh my goodness. Here we are on Independence Day, July 4th, 2024. That's how my brain works. It's got all the J months in one file folder. July 4th, Independence Day. And Anthropx had a very good couple weeks. And one of their models may be the smartest right now. But my general recommendation is still for someone's first $20, and if it's your only $20, you want to be at OpenAI's GPT+. One, it's the center of the universe. It's the center of gravity. And even though competitors are getting better, it's still the best bang for the buck. You're still going to get one of the best interfaces. It's easy to use. You can create your own prompts and you can share them with others. You get DALI, the image creation. You get the web browsing. You get the data analyst. You get code interpretation. So bang for the buck. OpenAI's ChatGPT+, their premium $20 a month plan, is still your first and best $20. If you're only going to drop your $20 somewhere, drop it there. And if you're not going to drop any money a few times a day getting a chance to use the features that OpenAI makes available for free in ChatGPT, if you're going to go nowhere else, it's a good place to go. The second, and this is largely a three-horse race right now. These are the ones that cost a billion dollars to get into the race. And the second, we've mentioned a couple times, is Anthropic's CLAWD. Anthropic is definitely number two. And in several ways, they do a couple things maybe even better on the edges than OpenAI. Their medium-sized model called CLAWD 3.5 Sonnet, it's the best. It's the smartest. It's the most intelligent of the AIs that mom and pop have access to right now. The other thing I really like about CLAWD is how natural-sounding its language is. That's one of the things I don't like and a lot of people don't like about ChatGPT is it's a little flowery. It tends to use words like a grade 10 with a thesaurus gone wrong. I find that the language that I get back out of CLAWD sounds much more natural to my ear. I have to do a whole lot less after-the-fact editing or prompt tweaking to try to get the language to match what it is that I want to come out of it as far as the word choice itself and the phrasing does. It's also very fast. It doesn't generate art the same way that DALI does, but it generates diagrams and graphs and lots of other things in a very nice way. This new feature that came out with the 3.5 Sonnet upgrade called Artifacts. In the previous story, I talked about how I'd taken my obituary analyst over to a CLAWD project to test it. One of the things that I was able to do was, after it actually read an obituary, I was able to very reliably ask it to take all of the relationships that it found in the obituary and turn it into a representation of a family tree in the artifact window. That was really useful. You think about how an obituary review goes into a workflow. If you're going to look at an obituary, make sure that you understand it deeply and then, say, put it into your ancestry tree after you've confirmed it. Being able to get a breakdown of all the relationships, as well as sort of a quick and dirty visual representation of the tree itself, that makes it so much easier and more reliable to be able to then take that and put it into ancestry. It just really reduces your error rate when you're going from obituary through to updated family tree. Yeah, I really like the Artifacts feature a lot. So then, if we give first place to OpenAI's ChatGPT and second place to Anthropic's Claude, the red-headed stepchild coming in third place is Google's Gemini. It's not dead last in all things. There's one or two situations where Google's Gemini is actually the best in class in one notable area, and that's just in how much information it can digest or process at one time. For OpenAI, GPT-4.0 can handle about 50 pages. Claude can handle about 75 pages. Gemini blows them away. Gemini can safely chew on almost 400 pages. That's eight times bigger than ChatGPT can do. You can see Google's experience with scaling big systems come out here. I mean, they've got 25 years under their belt that scales at a level that most companies can't even dream of. That really is one of the things that they're really, really good at. I guess I could find it not surprising. They're even talking in the future where Google is already using the term infinite context window, where they'll essentially be able to use and process anything that you can throw at it. I'm sure it will come for a price, but they'll have the capacity to do it, because they are the scale people. One of the things that's missing from Gemini is the ability to save and share prompts, except you can see it's kind of coming. They have a project called Notebook LM, as in Notebook Language Model. That could become reusable prompt. It's half-baked right now. We'll let that get a little bit more fully baked. There's one other feature that Google Gemini has, the ability to see other things that you store in other Google products. For example, it can see your Gmail. It can help you write a new mail message in Gmail. It can help you rephrase something directly within a Google Doc. So this is a functionality that's available, just because they're building the Gemini large language model into all of the Google Workspace products. They're not there yet, and some of the features are just rolling out. Even Steve and I noticed we both have a Google Workspace account. Some of us have features in one, some of us have features in another, and vice versa that we don't have. So they're in the middle of rollout, but this is something that will give features that are possible only in Gemini in the future that the other two can't touch yet. And that probably brings us to our Family History AI Tip of the Week. You want to introduce our Tip of the Week, Mark? So in this week's AI Tip of the Week, we're going to talk about how important it is to chat with your chatbot. There's a lot of information out there on the interwebs about prompt engineering and the best way to build a prompt. And we find one of the most important things to remember is to talk with it like it's another person. There's a thing that we have to unlearn that we've learned how to do in the last 30 or 40 years of working with software. And we've all learned how to kind of ask the magic question and get the answer back. And so we learn how to use Google. We learn how to phrase the perfect one answer before we click the submit button. We learn how to, you know, find the right command in the menu and then click go and then whatever it is that we want to have happen. All of us, myself included, we learn this term prompt engineering, like how to ask the question. And we work really, really hard to try to come up with the one question that gets us the best answer. And what I'm learning now is that's actually the wrong approach. It's rather than think about what's the one question that will get me the best answer. I'm now changing my thinking to how can I design a conversation the best way to get me the best answer at the end? I love that. And that's such a huge deal. I mean, that is the conceptual framework is that first we have to unlearn 30 years of Google rotting our brains. I loved how you phrased that as we're building a conversation. If you think about the whole chat itself as the product and that you're designing and building a conversation, you're helping it build up to something very useful. To me, that boils down to two things. You focus on the verbs. You're telling it what to do. You're talking to it as if it was an intelligent child or an intelligent intern. These are not human beings. And if you actually give them a series of simple commands, you can step them through a process. And by doing that, you're building up a context window to get a valuable result. Take like a really common prompt is role, goal, format. There's a really common prompt plan, right? Role, goal, format. For example, please act in the role of a genealogist. There's the role. Goal is help me analyze this obituary. And the format is please give me all your response in the table with all of the relationships identified. There's the simple one, two, three. And what a lot of people will do, and myself included, is we'll write that as a single prompt and then hit the enter key at the end. And the difference here is hit the enter key after every one of the steps. Learn to do that and get intermediate feedback. And it doesn't just understand it. It actually helps you build the prompt because it will respond with its understanding and fill in the prompt. Remember, every word in the prompt matters, whether you wrote it or the chatbot wrote it. It all becomes part of the prompt. So you can very quickly build up a very rich, detailed with examples prompt with very little effort if you actually invite the chatbot into that conversation by hitting the enter key through every step of the process. Well, that's a good tip of the week to just chat with your chatbot. Give it things in little manageable steps. Break it down step by step. And if you do those two things, that's how you get really effective and useful results. Let's move into AI RapidFire. I will start off with an update, two updates actually from OpenAI recently. I thought about this one as OpenAI giveth and OpenAI taketh away. So we'll start with the good news first. For those of our Mac users out there in family history AI land, ChatGPT has now released an app for Mac that you can install on your Mac desktop and run it like a local application. Not that different than what we currently run on the iOS app on our phones and on our iPads. So that's actually going to be, I think, a big win for our Mac users. I look forward to being able to do that not too far down the road on a Windows PC as well. Easy to interact with files because there's going to be more of that in the future. Easy to interact with pictures if you're uploading pictures or downloading pictures to be able to do it from within an app. So I think that's a big win. And in that app, you have free access to the GPT 4.0 model as well. So that's another big bonus. So I'm really happy to hear that. The one other thing, though, that on the OpenAI taketh away side, we've got a ChatGPT voice assistant has been delayed. Although it doesn't really sound to a lot of people like it's going to be super relevant to the genealogy world, I think it will be. I know that I use Siri and Alexa a lot while I'm actually sitting at my desk. It's for me, it's like another computer that I can ask a simple question of and get a simple answer from. I look forward to being able to have in my earbuds or on my desk on my phone to be able to ask a question of ChatGPT, a more complex question that I can ask of Siri or Alexa today. I know that OpenAI has had a bit of controversy with this whole Scarlett Johansson voice lookalike problem that they had a few months ago. So they're probably being pretty careful with rolling that out a little bit too early so that they don't have one more lawsuit on the pile of many lawsuits for them to deal with. I'm also looking forward to the voice interaction, to being able to chat with the chatbot actually verbally. And it's just so fast. That is going to be very useful when it gets here. But it's going to take a little getting used to, to see folks talking to themselves or talking to some AI that's in their earbud. But I think folks will see the usefulness of it. It'll be interesting to see if Apple manages to release the smarter Siri before OpenAI releases their voice assistant. Won't that be interesting to see if Apple actually beats them to the punch? It will be. Our second rapid fire item is from Google. We've talked about Google a couple times today. I learned about Google products that I didn't know I had access to until actually this week. Google announced that Gemini would be available inside Gmail and inside Google Drive, Google Docs, Google Sheets, places like that. Now AI is coming inside your Gmail and your Google Docs and your Google Sheets. And right now it is only included for folks who are paying for special AI access to Google's tools. And so I didn't even know I had access until I went and logged into that account and found that when I went to Gmail, there was a new Gemini button in the upper right corner. And it would summarize emails. It would process emails lots of different ways. I can imagine it would be very useful for summarizing long conversations with a particular person over a period of weeks or months or even years. This is one place where I think Google's got a shot to really distinguish itself because unlike the other chatbots, it's actually got access to your information. So it'll be able to do things that the other ones can't because it can actually help you with an email. It can help you with a document where you're working on it. Who hasn't come back to their email after sending out the big genetic genealogy messages to a bunch of your matches looking for help? You come back in and there's like 40 messages sitting in your inbox. And you have to read them all. And three quarters of them are not that useful. To be able to look through them all quickly, find all of the people who are actually interested in talking about this project that I'm working on, that'd be really, really useful. Save a lot of time rather than looking at 40 messages. The next rapid fire item comes from Time Magazine of all places. And in a new content licensing deal that they announced with OpenAI last week. I'm always really excited when I see these licensing deals for a few reasons. One of them is, is it lends some legitimacy to the AI business. There's a lot of people out there in society who rightfully are worried about what these AI companies are doing to journalism and to the web overall because of copyright issues. And we'll leave that to the courts to work out over the next five to ten years. But whenever we see one of these big content creators like Time Magazine, and before that, it was the Atlantic. Whenever we see them licensing their content to these AI companies, I get really excited because it legitimizes the fact that these AI companies are doing the right thing. But for us in particular, in genealogy, it does something really important. It makes available to these large language models, really high quality training data for the long term. Not only the training data that's available today and into the future, but also the training data that's in the past. These are professional writers with professional editors who are creating really, really high quality words to give to these large language models. That makes the large language models better. We've figured this out. The better the words are that go in, the smarter the large language models become. Now, in the case of Time Magazine, in particular for genealogy, this helps us in two ways. They didn't just say, you can use my information going into the future. They're actually licensing the entire back catalog of all Time Magazines back to like 100 years ago. So that information is going to be able to actually be surfaced and searched for, not just included in the training data, but actually those stories allowed to be found, which is great for research purposes. Deals like this actually directly benefit the genealogy community. On the other side, not only did Time license the rights to the content to OpenAI, OpenAI licensed technology to Time as part of the deal. So Time is going to be able to build in AI-based research tools and search tools into their own systems so that when people go into search through the back catalog of Time Magazine articles, they'll be able to use AI tools to search it directly from within Time. That's the hope. Better words make our large language models better. And the closer we can get AI tools to where we actually need them for genealogy research, the better off we'll be. I really like this story and other stories like it. We're going to keep looking for them. These licensing deals are a big deal, especially when we hear that it's with OpenAI. Some of the other companies don't need them as bad. Facebook has all Facebook's data, so they've got plenty. Amazon's got all their reviews, and that's a lot for their models to train on. Google has all of our email and our documents, so they've got lots of training data. But OpenAI doesn't have all its own, and so they've gotten in a little bit of sketchy water for scraping places maybe they shouldn't have. So these legitimate licensing deals are important, especially for them. Another licensing deal, our last rapid-fire item, concerns audio. One of the tasks that I have used in the past, I love podcasts, and I love listening to things. I spend a lot of my time on the ride and lawn mower listening to either podcasts or audiobooks. But there are some things you can't get in audio. You might just have long articles. And, for example, I'll find a 20-page article that I want to read, but I won't have time to read it. Well, for several years, I've been turning those into my own little audio files, but they didn't sound very good. They sounded very robotic. You know, we've been able to turn text into speech for 20 or 30 years, but it just sounded awful. Sounds stilted. Yes. But in the past year or two, text to audio has gotten very, very good. And one of the premier companies at this is called Eleven Labs. Eleven Labs went to a Hollywood agency that represents not living actors and actresses, but they found an agency that actually represents the estates of some deceased actors and actresses. And so they've licensed the voice of Judy Garland, James Dean, Burt Reynolds, and Sir Lawrence Olivier to be used for text to speech. And so now if you have a big chunk of text that you want to have read to you by Judy Garland, James Dean, Burt Reynolds, or Sir Lawrence Olivier, you can do that. And for our younger folks who may not know who they are— Ask your mom. Yes, yes, yes. Burt Reynolds, I just think of Smokey and the Bandit. I don't know where—you know, that's Burt Reynolds for me. And Judy Garland, of course, is from The Wizard of Oz, James Dean. There's a really famous James Dean poster that I always see when I think of James Dean. Rebel Without a Cause, maybe? Rebel Without a Cause. There we go. Thank you, sir. And I think the idea of being able to, quote unquote, read a large document while I'm out for my walk or while I'm working on, you know, while I'm cleaning the dishes or while I'm, you know, sweeping my floors, I think that's really useful. You know, there's another thing, too, that I think is useful is there's a lot of people who are quite a bit older compared to a lot of industries. And along with that comes failing eyesight. My relative who got me into genealogy said the great irony of genealogy is that by the time that people get interested in genealogy, they have a hard time reading the documents that they want to research. And so the ability to be able to have reading support for some documents that are difficult to read, I think, is a great idea. There's going to be a lot of genealogical use cases for text to audio that are much less creepy than video. We've seen that there's been a couple companies that have come out, they can take a photograph of a deceased ancestor and animate that. And that freaks out some people to see someone who is deceased apparently moving. It's a little strange, but voice is a little different. And so here's a couple use cases that I think folks are going to find less creepy and more enjoyable. For example, imagine you have audio tape or videotape of a grandparent or great-grandparent from a family reunion in the 50s, 60s, or 70s, and so you have their voice. And all you need is about 20 seconds of their voice, and then you can clone their voice so that if you found a letter from your great-grandfather, you could actually have him apparently read a letter in his own voice. And I think folks would find that less creepy than video. The cloned voices are really real. Yeah, I appreciate, just kind of like the previous rapid fire, I appreciate that Eleven Labs found a way to do this right and to choose some names that are engaging and interesting and deceased so there isn't going to be any misuse with real persons, with living persons. And some names that I think a lot of people will identify with, and the voices themselves are really recognizable. You know, James Dean and Burt Reynolds for me, you know, it's the same. Burt Reynolds for me is Smokey and the Bandit. So I can, you know, when I hear him talking, when I did the examples on the Eleven Labs app, it was like I'm back, I'm Smokey and the Bandit, you know, in 1977 or whatever it was. Sir Lawrence Olivier, if you wanted to sound a bit more scholastic perhaps, the British accent? Yeah, Sir Lawrence Olivier is probably a better person to read your research report than Burt Reynolds. Yes, yes. Well, I think that's a wrap for this week, Steve. Thank you very much. Lots of news this week. It was nice to be able to go a little bit deeper into a few features really for the genealogy community. So that was very enjoyable. I hope everybody out there had a great Independence Day and Canada Day and I look forward to catching up with you after my big family reunion this coming weekend. Hopefully we'll have some great stories from the Thompson Family Reunion, which is scheduled for this coming weekend. Have a great reunion, Mark. We'll talk afterwards. Great, thanks. Bye-bye.
</RAW TRANSCRIPT>
</TRANSCRIPT>


